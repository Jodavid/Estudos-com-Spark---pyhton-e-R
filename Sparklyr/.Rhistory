Sys.getenv("$SPARK_HOME")
Sys.getenv("/home/hartbjf/spark-3.1.2-bin-hadoop3.2/")
#' -----------------------
#' Fazendo a conexão para o Spark Local
#' -----------------------
sc <- spark_connect(master = "local",
spark_home = "/home/hartbjf/spark-3.1.2-bin-hadoop3.2/")
?spark_read_parquet
#' -----------------------
#' Lendo arquivos parquet
#' -----------------------
dados <- spark_read_parquet(sc, name = "../banco de dados - exemplo/userdata/" )
#' -----------------------
#' Verificando número de linhas
#' -----------------------
dim(dados)
library(dplyr)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
str(dados)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
names(dados)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
sdf_nrow(dados)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
sdf_nrow(dim)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
sdf_dim(dados)
sdf_nrow(dados)
sdf_ncol(dados)
# ------------------------------------
# Verificando as colunas
# -----------------
colnames(dados)
?srt
?str
?typeof
sdf_schema(dados)
#' -------------------------------------------------------
#' Estudos com sparklyr + dados parquet
#'
#' Criado por: Jodavid Ferreira
#' Data de criação: 02.07.2021
#'
#' Objetivo: Avançar nos estudos com R + spark
#' -------------------------------------------------------
#' -----------------------
#' Instalando o spark
#' -----------------------
# install.packages("sparklyr")
#' -----------------------
#' 1 Lendo as bibliotecas necessárias
#' -----------------------
library(sparklyr)
library(dplyr)
#' -----------------------
#' ----------------------------------------------------------------------------
#' -----------------------
#' 2 Fazendo a conexão para o Spark Local
#' -----------------------
sc <- spark_connect(master = "local",
spark_home = "/home/hartbjf/spark-3.1.2-bin-hadoop3.2/")
#' -----------------------
#' Lendo arquivos parquet
#' -----------------------
dados <- spark_read_parquet(sc, name = "../banco de dados - exemplo/userdata/" )
#' -----------------------
#' Verificando número de linhas
#' -----------------------
sdf_dim(dados)
sdf_nrow(dados)
sdf_ncol(dados)
#' ------------------------------------
#' Verificando as colunas
#' -----------------
colnames(dados)
sdf_schema(dados)
#' -----------------
#' ----------------------------------------------------------------------------
#' ------------------------------------
#' 3 Análise Exploratória - Descritiva
#' -----------------
dados |>
select(colunas_selecionadas)
#' ------------------------------------
#' 3 Análise Exploratória - Descritiva
#' -----------------
colunas_selecionadas <- "salary"
dados |>
select(colunas_selecionadas)
dados |>
select(colunas_selecionadas) |>
summary()
dados |>
select(colunas_selecionadas) |>
sdf_describe()
dados |>
select(colunas_selecionadas)
dados |>
select(colunas_selecionadas) |>
mean()
dados |>
select(colunas_selecionadas) |>
sd()
#' ------
spark_dataframe(dados)
#' ---
library(sparklyr.flint)
install.packages("sparklyr.flint")
#' ---
library(sparklyr.flint)
#' ------
dados |>
select(colunas_selecionadas) |>
summarize_kurtosis()
summarize_kurtosis(dados, column = colunas_selecionadas)
summarize_kurtosis(ts, column = "salary")
summarise_kurtosis
?summarise_kurtosis
?summarize_kurtosis
summarize_kurtosis(sc, column = "salary")
#' ------
dados |>
select(colunas_selecionadas) |>
skewness()
dados[[1]]
dados[[2]]
#' ------
dados |>
select(colunas_selecionadas) |>
sdf_describe()
#' ------
dados |>
select(colunas_selecionadas) |>
skewness()
#' ------
dados |>
select(colunas_selecionadas) |>
summarize_kurtosis()
#' ------
dados |>
select(colunas_selecionadas) |>
summarize_kurtosis(., column = colunas_selecionadas)
#' ------
dados2 <- fromSDF(dados)
#' ---
library(sparklyr.flint)
)
price_sdf <- copy_to(
sc,
data.frame(
time = ceiling(seq(12) / 2),
price = seq(12) / 2,
id = rep(c(3L, 7L), 6)
))
ts <- fromSDF(price_sdf, is_sorted = TRUE, time_unit = "DAYS")
ts
ts <- fromSDF(price_sdf, is_sorted = TRUE, time_unit = "DAYS")
#' -----------------------
#' 1 Lendo as bibliotecas necessárias
#' -----------------------
library(sparklyr)
library(dplyr)
#' ---
library(sparklyr.flint)
#' -----------------------
#' 2 Fazendo a conexão para o Spark Local
#' -----------------------
#sc <- spark_connect(master = "local",
#                    spark_home = "/home/hartbjf/spark-3.1.2-bin-hadoop3.2/")
sc <- try_spark_connect(master = "local",
spark_home = "/home/hartbjf/spark-3.1.2-bin-hadoop3.2/")
#' -----------------------
#' Lendo arquivos parquet
#' -----------------------
dados <- spark_read_parquet(sc, name = "../banco de dados - exemplo/userdata/" )
#' -----------------------
#' Verificando número de linhas
#' -----------------------
sdf_dim(dados)
sdf_nrow(dados)
sdf_ncol(dados)
#' ------------------------------------
#' Verificando as colunas
#' -----------------
colnames(dados)
sdf_schema(dados)
#' ------------------------------------
#' 3 Análise Exploratória - Descritiva
#' -----------------
colunas_selecionadas <- "salary"
#' ------
dados |>
select(colunas_selecionadas) |>
sdf_describe()
price_sdf <- copy_to(
sc,
data.frame(
time = ceiling(seq(12) / 2),
price = seq(12) / 2,
id = rep(c(3L, 7L), 6)
))
ts <- fromSDF(price_sdf, is_sorted = TRUE, time_unit = "DAYS")
dados |>
select(colunas_selecionadas)
dados |>
select(colunas_selecionadas) |>
sdf_describe()
?sdf_describe
sdf_rnorm(100)
sdf_rnorm(100)
?sdf_rnorm
dados |>
select(colunas_selecionadas)
dados |>
select(colunas_selecionadas) |>
sdf_len(sc, 10) %>% spark_apply(function(df) df * 10)
sdf_len(sc, 10)
#' ------
dados |>
select(colunas_selecionadas) |>
spark_apply(function(x) mean(x))
#' ------
dados |>
select(colunas_selecionadas) |>
spark_apply(function(x) mean(x, na.rm=T))
?mean()
#' ------
dados |>
select(colunas_selecionadas) |>
spark_apply(function(x) summary(x) )
#' ------
dados |>
select(colunas_selecionadas) |>
spark_apply(function(x) sd(x, na.rm = TRUE) )
#' ------
dados |>
select(colunas_selecionadas) |>
spark_apply(function(x) mean(x, na.rm = TRUE) )
sdf_read_column(dados, column = "salary")
mean(sdf_read_column(dados, column = "salary"), na.rm = T)
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
select(colunas_selecionadas) |>
sdf_read_column()
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
select(colunas_selecionadas) |>
sdf_read_column(column = .)
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
select(colunas_selecionadas) |>
sdf_read_column(column = "colunas_selecionadas")
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
#select(colunas_selecionadas) |>
sdf_read_column(.,column = "colunas_selecionadas")
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
#select(colunas_selecionadas) |>
sdf_read_column(*,column = "colunas_selecionadas")
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
#select(colunas_selecionadas) |>
sdf_read_column(dados,column = "colunas_selecionadas")
sdf_read_column(dados,column = "colunas_selecionadas")
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
#select(colunas_selecionadas) |>
sdf_read_column(dados,column = "salary")
sdf_read_column(dados,column = "salary")
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
#select(colunas_selecionadas) |>
sdf_read_column(column = "salary")
#' ---
library(e1071) # Para Skewness e Kurtosis
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
sdf_read_column(column = "salary") |>
skewness()
?skewness()
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
sdf_read_column(column = "salary") |>
?skewness(na.rm = TRUE)
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
sdf_read_column(column = "salary") |>
skewness(na.rm = TRUE)
#' -----
dados |>
sdf_read_column(column = "salary") |>
kurtosis(na.rm = TRUE)
#' ------------------------------------
#' Gerando um histograma
#' -----------------
x <- dados %>% select('salary') %>% collect()
x
hist(salary$salary)
#' ------------------------------------
#' Gerando um histograma
#' -----------------
salary <- dados %>% select('salary') %>% collect()
hist(salary$salary)
remotes::install_github("GabeChurch/sparkedatools")
library(sparkedatools)
#' ------
spark_table = tbl(sc, sql("select * from dados.salary limit 100"))
#' ------
spark_table = tbl(dados, sql("select * from salary limit 100"))
#' ------
spark_table = tbl(sc, sql("select salary from dados limit 100"))
#' ------
spark_table = tbl(sc, sql("select salary from dados"))
#' ------
spark_table = tbl(sc, sql("SELECT * FROM dados.columns"))
#' ------
spark_table = tbl(sc, sql("SELECT * FROM dados.salary"))
#' ------
spark_table = tbl(sc, sql("SELECT * FROM dados"))
?sql
#' ------
spark_table = tbl(sc, sql("select * from dados"))
View(dados)
#' ------------------------------------
#' Verificando as colunas
#' -----------------
colnames(dados)
#' ------
spark_table = tbl(sc, sql("select * from dados.salary"))
#' ------
spark_table = tbl(dados, sql("select * from dados.salary"))
#' ------
spark_table = tbl(dados, sql("select * from salary"))
salary
#' ------
# plot
ggplot(data = salary, aes(x = salary)) +
+ geom_histogram()
library(ggplot2)
#' ------
# plot
ggplot(data = salary, aes(x = salary)) +
+ geom_histogram()
#' ------
# plot
ggplot(data = salary, aes(x = salary)) +
geom_histogram()
#' ------
# plot
ggplot(data = salary, aes(x = salary)) +
geom_histogram(color="black")
hist(salary$salary)
#' -------------------------------------------------------
#' Estudos com sparklyr + dados parquet
#'
#' Criado por: Jodavid Ferreira
#' Data de criação: 02.07.2021
#'
#' Objetivo: Avançar nos estudos com R + spark
#' -------------------------------------------------------
#' -----------------------
#' Instalando o spark
#' -----------------------
# install.packages("sparklyr")
# remotes::install_github("GabeChurch/sparkedatools")
#' -----------------------
#' 1 Lendo as bibliotecas necessárias
#' -----------------------
library(sparklyr)
library(dplyr)
#' ---
library(e1071) # Para Skewness e Kurtosis
library(sparkedatools)
library(ggplot2)
#' -----------------------
#' ----------------------------------------------------------------------------
#' -----------------------
#' 2 Fazendo a conexão para o Spark Local
#' -----------------------
#sc <- spark_connect(master = "local",
#                    spark_home = "/home/hartbjf/spark-3.1.2-bin-hadoop3.2/")
sc <- try_spark_connect(master = "local",
spark_home = "/home/hartbjf/spark-3.1.2-bin-hadoop3.2/")
#' -----------------------
#' Lendo arquivos parquet
#' -----------------------
dados <- spark_read_parquet(sc, name = "../banco de dados - exemplo/userdata/" )
#' -----------------------
#' Verificando número de linhas
#' -----------------------
sdf_dim(dados)
sdf_nrow(dados)
sdf_ncol(dados)
#' ------------------------------------
#' Verificando as colunas
#' -----------------
colnames(dados)
sdf_schema(dados)
#' -----------------
#' ----------------------------------------------------------------------------
#' ------------------------------------
#' 3 Análise Exploratória - Descritiva
#' -----------------
colunas_selecionadas <- "salary"
#' ------
dados |>
select(colunas_selecionadas) |>
sdf_describe()
#' -----------------
#' ------------------------------------
#' Assimetria e Curtose
#' -----------------
dados |>
sdf_read_column(column = "salary") |>
skewness(na.rm = TRUE)
#' -----
dados |>
sdf_read_column(column = "salary") |>
kurtosis(na.rm = TRUE)
#' -----------------
#' ------------------------------------
#' Gerando um histograma
#' -----------------
salary <- dados %>% select('salary') %>% collect()
hist(salary$salary)
#' ------
# plot
ggplot(data = salary, aes(x = salary)) +
geom_histogram(color="black")
#' ------------------------------------
#' Gerando um histograma
#' -----------------
salary <- dados |>
select('salary') |>
collect()
#' ------
ggplot(data = salary, aes(x = salary)) +
geom_boxplot(color="black")
#' ------
ggplot(data = salary, aes(y = salary)) +
geom_boxplot(color="black")
#' ------
ggplot(data = salary, aes(y = salary)) +
geom_violin(color="black")
#' ------
ggplot(data = salary, aes(x = salary)) +
geom_violin(color="black")
#' ------
ggplot(data = salary, aes(x = salary, y = salary)) +
geom_violin(color="black")
#' ------
ggplot(data = salary, aes(x = salary, y = salary)) +
geom_violin(color="black") +
geom_boxplot(width=0.1)
#' ------
ggplot(data = salary, aes(x = salary, y = salary)) +
geom_violin(color="black") +
geom_boxplot(width=0.01)
#' ------
ggplot(data = salary, aes(x = salary, y = salary)) +
geom_violin(color="black") +
geom_boxplot(width=0.01)
#' ------
ggplot(data = salary, aes(x = salary, y = salary)) +
geom_violin(color="black") +
geom_boxplot(width=10)
#' ------
ggplot(data = salary, aes(x = salary, y = salary)) +
geom_violin(color="black") +
geom_boxplot(width=2,color="red")
#' ------
ggplot(data = salary, aes(x = salary, y = salary)) +
geom_violin(color="black")
ggplot(data = salary, aes(x = salary, y = salary)) +
geom_violin(trim=FALSE, fill='#A4A4A4', color="darkred")+
geom_boxplot(width=0.1) + theme_minimal()
#' ------
ggplot(data = salary, aes(x = salary, y = salary)) +
geom_violin(color="black")
