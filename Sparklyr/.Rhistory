#' -----------------------
#' Instalando o spark
#' -----------------------
install.packages("sparklyr")
#' -----------------------
#' Lendo as bibliotecas necessárias
#' -----------------------
library(sparklyr)
?spark_connect
#' -----------------------
#' Fazendo a conexão para o Spark Local
#' -----------------------
sc <- spark_connect(master = "local")
#' -----------------------
#' Fazendo a conexão para o Spark Local
#' -----------------------
sc <- spark_connect(master = "local",
spark_home = Sys.getenv("SPARK_HOME"))
Sys.getenv("SPARK_HOME")
Sys.getenv("$SPARK_HOME")
Sys.getenv("/home/hartbjf/spark-3.1.2-bin-hadoop3.2/")
#' -----------------------
#' Fazendo a conexão para o Spark Local
#' -----------------------
sc <- spark_connect(master = "local",
spark_home = "/home/hartbjf/spark-3.1.2-bin-hadoop3.2/")
?spark_read_parquet
#' -----------------------
#' Lendo arquivos parquet
#' -----------------------
dados <- spark_read_parquet(sc, name = "../banco de dados - exemplo/userdata/" )
#' -----------------------
#' Verificando número de linhas
#' -----------------------
dim(dados)
library(dplyr)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
str(dados)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
names(dados)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
sdf_nrow(dados)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
sdf_nrow(dim)
#' -----------------------
#' Verificando número de linhas
#' -----------------------
sdf_dim(dados)
sdf_nrow(dados)
sdf_ncol(dados)
# ------------------------------------
# Verificando as colunas
# -----------------
colnames(dados)
?srt
?str
?typeof
sdf_schema(dados)
